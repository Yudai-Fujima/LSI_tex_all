
@misc{radford_unsupervised_2016,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	doi = {10.48550/arXiv.1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	urldate = {2026-01-12},
	publisher = {arXiv},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = jan,
	year = {2016},
	note = {arXiv:1511.06434 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Under review as a conference paper at ICLR 2016},
	file = {Full Text PDF:C\:\\Users\\yoshi\\Zotero\\storage\\VZBY5F76\\Radford et al. - 2016 - Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.pdf:application/pdf;Snapshot:C\:\\Users\\yoshi\\Zotero\\storage\\MDDWD9CA\\1511.html:text/html},
}

@misc{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	doi = {10.48550/arXiv.1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2026-01-12},
	publisher = {arXiv},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv:1406.2661 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Full Text PDF:C\:\\Users\\yoshi\\Zotero\\storage\\4X8C2HCU\\Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;Snapshot:C\:\\Users\\yoshi\\Zotero\\storage\\NUJRHIZA\\1406.html:text/html},
}

@article{vershynin_introduction_nodate,
	title = {An {Introduction} with {Applications} in {Data} {Science}},
	language = {en},
	author = {Vershynin, Roman},
	file = {PDF:C\:\\Users\\yoshi\\Zotero\\storage\\GVB8RYGH\\Vershynin - An Introduction with Applications in Data Science.pdf:application/pdf},
}

@article{liu_application_2022,
	title = {Application of an {Improved} {DCGAN} for {Image} {Generation}},
	volume = {2022},
	copyright = {Copyright © 2022 Bingqi Liu et al.},
	issn = {1875-905X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/9005552},
	doi = {10.1155/2022/9005552},
	abstract = {With the rapid development of deep learning, image generation technology has become one of the current hot research areas. A deep convolutional generative adversarial network (DCGAN) can better adapt to complex image distributions than other methods. In this paper, based on a traditional generative adversarial networks (GANs) image generation model, first, the fully connected layer of the DCGAN is further improved. To solve the problem of gradient disappearance in GANs, the activation functions of all layers of the discriminator are LeakyReLU functions, the output layer of the generator uses the Tanh activation function, and the other layers use ReLU. Second, the improved DCGAN model is verified on the MNIST dataset, and simple initial fraction (ISs) and complex initial fraction (ISc) indexes are established from the two aspects of image quality and image generation diversity, respectively. Finally, through a comparison of the two groups of experiments, it is found that the quality of images generated by the DCGAN model constructed in this paper is 2.02 times higher than that of the GANs model, and the diversity of the images generated by the DCGAN is 1.55 times higher than that of GANs. The results show that the improved DCGAN model can solve the problem of low-quality images being generated by the GANs and achieve good results.},
	language = {en},
	number = {1},
	urldate = {2026-01-13},
	journal = {Mobile Information Systems},
	author = {Liu, Bingqi and Lv, Jiwei and Fan, Xinyue and Luo, Jie and Zou, Tianyi},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1155/2022/9005552},
	pages = {9005552},
	file = {Full Text PDF:C\:\\Users\\yoshi\\Zotero\\storage\\T47YQ2HP\\Liu et al. - 2022 - Application of an Improved DCGAN for Image Generation.pdf:application/pdf;Snapshot:C\:\\Users\\yoshi\\Zotero\\storage\\RR6HTPJM\\9005552.html:text/html},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2026-01-14},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Fixes a typo in the abstract, no other changes},
	file = {Full Text PDF:C\:\\Users\\yoshi\\Zotero\\storage\\UZPFLQ87\\Kingma と Welling - 2022 - Auto-Encoding Variational Bayes.pdf:application/pdf;Snapshot:C\:\\Users\\yoshi\\Zotero\\storage\\TP7H4AUR\\1312.html:text/html},
}

@article{yee_proteus_2007,
	title = {The {Proteus} {Effect}: {The} {Effect} of {Transformed} {Self}-{Representation} on {Behavior}},
	volume = {33},
	issn = {1468-2958},
	shorttitle = {The {Proteus} {Effect}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-2958.2007.00299.x},
	doi = {10.1111/j.1468-2958.2007.00299.x},
	abstract = {Virtual environments, such as online games and web-based chat rooms, increasingly allow us to alter our digital self-representations dramatically and easily. But as we change our self-representations, do our self-representations change our behavior in turn? In 2 experimental studies, we explore the hypothesis that an individual’s behavior conforms to their digital self-representation independent of how others perceive them—a process we term the Proteus Effect. In the first study, participants assigned to more attractive avatars in immersive virtual environments were more intimate with confederates in a self-disclosure and interpersonal distance task than participants assigned to less attractive avatars. In our second study, participants assigned taller avatars behaved more confidently in a negotiation task than participants assigned shorter avatars. We discuss the implications of the Proteus Effect with regards to social interactions in online environments.},
	language = {en},
	number = {3},
	urldate = {2026-01-21},
	journal = {Human Communication Research},
	author = {Yee, Nick and Bailenson, Jeremy},
	year = {2007},
	pages = {271--290},
	file = {Full Text PDF:C\:\\Users\\yoshi\\Zotero\\storage\\MQ5JQ96I\\Yee と Bailenson - 2007 - The Proteus Effect The Effect of Transformed Self-Representation on Behavior.pdf:application/pdf;Snapshot:C\:\\Users\\yoshi\\Zotero\\storage\\JJWDUUP2\\j.1468-2958.2007.00299.html:text/html},
}


@techreport{soumusho2024_information_communications_whitepaper,
  title        = {令和6年版 情報通信白書},
  author       = {{総務省 情報通信白書編集委員会}},
  url          = {https://www.soumu.go.jp/johotsusintokei/whitepaper/ja/r06/pdf/00zentai.pdf},
}


@manual{amd_ug1267_zcu104,
  title        = {{ZCU104 Evaluation Board User Guide}},
  author       = {{Advanced Micro Devices, Inc.}},
  organization = {{AMD}},
  url          = {https://docs.amd.com/v/u/en-US/ug1267-zcu104-eval-bd},
}


@manual{amd_pynq,
  title        = {{What is PYNQ}},
  author       = {{Advanced Micro Devices, Inc.}},
  organization = {{AMD}},
  url          = {https://www.pynq.io/},
}
