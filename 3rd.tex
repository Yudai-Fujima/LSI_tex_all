\newpage
\section{準備}
本章では、4.1において生成モデルにおけるGANの位置づけを示し、4.2においてGANの学習アルゴリズムについて説明する。

\subsection{生成モデル}
生成モデルは未知の真の分布$p_{data}(x)$をモデル分布$p_{\theta}$で近似することを目標とする。%（モデルのパラメータを$\theta$としたとき、$x_i$が確率$p(x;\theta)$から生成されることを$x_i\sim p(x;\theta)$あるいは$p_{\theta}$と書く。）
これは、モデルから画像$x$が生成される確率である周辺尤度$p_\theta(x)=\int p_\theta(z)p_\theta(x|z)dz$を最大化することに等しいが、高次元空間において、この$p_\theta(x)$を直接的に計算することは難しい。この問題に対処したアルゴリズムとしてVAEとGANがある。

% しかしながら、画像のように高次元なデータの場合、$p_{data}(x)$が高次元で複雑な意味構造をもつため、これを$p_\theta$で直接的に近似することは難しい。そこで、$p_{data}(x)$の情報を低次元に落とし込むための潜在変数$z$を導入する。このとき、潜在変数$z$は事前分布$p_\theta(z)$に従って生成され、その後、条件付き分布$p_\theta(x|z)$に従って新たな$x$を生成する。しかし、真のパラメータ$\theta$および各データ点に対する潜在変数$z$の値は観測できず未知である。



\subsubsection{VAE}
画像データ$x$の特徴を潜在変数として次元の圧縮を行い、その潜在変数に基づいて画像を生成する仕組みをオートエンコーダ―(AE)という。AEでは潜在変数と出力画像は一対一対応だが、潜在変数を確率分布とすることで未知データの出力を可能とした技術としてVAE(Variational Auto Encoder)\cite{kingma_auto-encoding_2022}がある。(\fref{fg:VAE})

% VAEの目標はモデルから画像$x$が生成される確率$p_\theta(x)=\int p_\theta(z)p_\theta(x|z)dz$を求めることであるが、この$p_\theta(x)$を直接的に計算することは難しい。そこで、VAEでは潜在変数$z$を介して、推論モデル$q(z|x)$、生成モデル$p(x|z)$を用いる。

% VAEの推論モデル$q(z|x)$では、入力データの平均$\mu$と分散$\sigma^2$を求め、潜在空間$N(\mu,\sigma^2)$の推定をおこなう。生成モデル$p(x|z)$では、潜在変数$z$を潜在空間$N(\mu,\sigma^2)$からランダムにサンプリングすることで、新しい画像を生成することが可能となる。
VAEでは$p_\theta(x)$を直接的に計算することは難しいという問題に対して、潜在変数$z$を介して、推論モデル$q_\phi(z|x)$および生成モデル$p_\theta(x|z)$を導入することで対処している。

VAEの推論モデル$q_\phi(z|x)$は、入力データ$x$に対して潜在変数の分布を近似するものであり、ニューラルネットワークを用いて平均$\mu$と分散$\sigma^2$を出力することで，$z$が従うガウス分布$\mathcal{N}(\mu,\sigma^2)$を定義する。生成モデル$p_\theta(x|z)$では、潜在変数$z$を事前分布$p(z)$からサンプリングし、それに基づいて新しい画像を生成することが可能となる。

VAEでは$p(x|z)$をガウス分布としてモデル化するため、再構成誤差は二乗誤差に一致する。しかしながら、二乗誤差は多峰的な分布に対して平均化を促す性質がある。その結果、高周波成分を正確に再現する鋭い画像よりも、平滑化された曖昧な画像の方が誤差が小さくなりやすく、VAEでは生成画像がぼやけやすいというアルゴリズム上の課題が存在する。


\begin{figure}[htbp]
	\centering
	\includegraphics[width=60mm,keepaspectratio]{figure/VAE.png}
	\caption{VAEの概要図}
	\label{fg:VAE}
\end{figure}

\subsubsection{GAN}
VAEと異なり，GAN\cite{goodfellow_generative_2014}は明示的な尤度関数$p_\theta(x)$や潜在変数の事後分布$q(z|x)$を定義せず、識別モデルを介して生成分布とデータ分布との差異を評価することで、分布全体を暗黙的に近似する生成モデルである。(\fref{fg:GAN_VAE})

具体的には、Generatorとよばれる生成モデル$G(z)$とDiscriminatorと呼ばれる識別モデル$D(x)$を用いて、これらを敵対的に学習させることで画像のデータ分布を学習する。

生成モデル$G(z)$は潜在変数$z$を画像空間に写像するパラメータを学習し、識別モデル$D(x)$は入力画像$x$が正解データか$G(z)$かを判別する関数として機能する。VAEとは異なり、$p_{\text{data}}、p_{\theta}$のJSダイバージェンスを最小化するように学習が進むため、高周波成分も再現したシャープな画像が生成されやすい。


\begin{figure}[htbp]
	\centering
	\includegraphics[width=60mm,keepaspectratio]{figure/GAN_VAE.png}
	\caption{GANの概要図(VAE比較)}
	\label{fg:GAN_VAE}
\end{figure}







\newpage

\subsection{GAN (Generative Adversarial Networks)の学習アルゴリズム}

GANの目標は画像データ$x$に対する生成モデルの分布$p_g$を学習することである。そのために、まず潜在変数$z$に対する事前分布$p_z(z)$を定義し、これをデータ空間へ移す写像として$G(z;\theta_g)$を学習する。$\theta_g$はNNのパラメータである。これにより、単純な事前分布$z\sim p_z(z)$を関数$G(z)$に入力することで、新たなデータ分布$x=p_g$を生成できる。

次に、単一のスカラー値を出力するパーセプトロン$D(x;\theta_d)$を定義する。$D(x)$は、入力$x$が生成分布$p_g$からではなく、実データ分布から得られたものである確率を表す。


\begin{figure}[htbp]
	\centering
	\includegraphics[width=65mm,keepaspectratio]{figure/GAN.png}
	\caption{GANの概要図}
	\label{fg:GAN}
\end{figure}

GANでは、\eref{eq:minimax}に示すminimax問題を解くことにより、$G(z)$はデータ分布を学習し、$D$は正解画像のデータ分布と$p_g$の判別方法を学習する。
\begin{align}
&\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{\text{data}}(x)} \left[ \log D(x) \right] \notag\\
&+ \mathbb{E}_{z \sim p_{z}(z)} \left[ \log (1 - D(G(z))) \right] \label{eq:minimax}
\end{align}

ここで、先に$D$の最適化を完全に進めてしまうと、$D(G(z))\approx 0$となってしまい、$G$の損失関数$L_G=\mathbb{E}_{z\sim p_z(z)}\left[\log(1 - D(G(z)))\right]$の勾配は$\partial \mathcal{L}_G / \partial \theta_g \approx 0$となる。その結果、$G$の有効な更新ができず、生成器$G$が$p_z(z)$から$p_g$へと変換する過程を学習できなくなってしまう。そのため、$D$と$G$を交互に更新することにより、$D$の過度な最適化を避け、$G$の学習を有効に進めることができる。

4.2.1、4.2.2にてDiscriminatorおよびGeneratorの最適解について、4.2.3にてGAN全体の学習の流れについて詳細を説明する。



\subsubsection{Discrminator最適解}
期待値$E$は確率変数$x$に対して関数$f(x)$の平均をとることで求まるから、離散の確率密度関数$P(x)$に対して$E[f(x)]=\Sigma f(x)P(x)$、連続の確率密度関数$p(x)$場合は$E[f(x)]=\int f(x)p(x)dx$と記述できる。

識別器Dでは、\eref{eq:minimax}の最大化を目指すため、\eref{eq:minimax}を変形すると\eref{eq:Dmax}のようになる。
{\footnotesize
\begin{align}
&V(D, G) = \notag\\
&\mathbb{E}_{x \sim p_{\text{data}}(x)} \left[ \log D(x) \right] + \mathbb{E}_{z \sim p_{z}(z)} \left[ \log (1 - D(G(z))) \right]\notag\\
&=\int dx\left[p_{\text{data}}(x)\log D(x)+ p_g(x)\log(1-D(x)) \right] \label{eq:Dmax}
\end{align}
}

ここで、関数$y=a\log y+b\log(1-y)$は$y=a/(a+b)$で最大値をとることから、\eref{eq:Dmax}は\eref{eq:D*}で最大値をとる。
\begin{align}
    D^*_G(x)=\dfrac{p_{\text{data}}(x)}{p_{\text{data}}(x)+p_g(x)}\label{eq:D*}
\end{align}

よって、識別器の最適解は$D^*_G(x)$と分かる。GANの学習が理想的に進行した場合、生成分布は実データ分布に一致し$p_{\text{data}}=p_g$となる。このとき、識別器の最適解は$D^*_G(x)=1/2$である。


\subsubsection{Generatorの最適解}


\subsubsection*{KLダイバージェンス}
確率分布の類似性を測る指標としてKLダイバージェンスがあり、真の確率分布$p(x)$を$q(x|\theta)$で表現することは、\eref{eq:KL}に示すKLダイバージェンスを最小化する問題に帰着することができる。
\begin{align}
    D_{KL}(p||q)=\int p(x)\log\dfrac{p(x)}{q(x)}dx \label{eq:KL}
\end{align}

\subsubsection*{JSダイバージェンス}
KLダイバージェンスは非対称性を持ち、$D_{KL}(P||Q)\neq D_{KL}(Q||P)$であるため距離として扱うことはできない。そこで、対称性を持たせるための指標としてJSダイバージェンスがあり、\eref{eq:JS}で示す。
\begin{align}
    D_{JS}(p||q)=\dfrac{D_{KL}(p||m)+D_{KL}(q||m)}{2} \label{eq:JS}
\end{align}

生成器の学習では、\eref{eq:Dmax}の$V(D,G)$の最小化を考える。\eref{eq:Dmax}の$V(D,G)$に対して、\eref{eq:D*}の最適演算子$D^*$および\eref{eq:KL}、\eref{eq:JS}を適用すると\eref{eq:VG}になる。
{\small
\begin{align}
    &V(G)=\int p_{\text{data}}(x)\log{\dfrac{p_{\text{data}}}{p_{\text{data}}+p_g}}dx\notag\\
    &　　+\int p_g(x)\log\left(1-\dfrac{p_{\text{data}}}{p_{\text{data}}+p_g}\right)dx\notag\\
    &=D_{KL}(p_{\text{data}}||(p_{\text{data}}+p_g))+\notag\\
    &　　D_{KL}(p_{\text{data}}||(p_{\text{data}}+p_g))-\log4\notag\\
    &=2D_{JS}(p_{\text{data}}||p_g)-\log4 \label{eq:VG}
\end{align}
}
\eref{eq:VG}から、生成器の学習はJSダイバージェンスの最小化問題に帰着できることが分かる。JSダイバージェンスは非負性を持つため、$D_{JS}\geq0$である。また、等式が成り立つのは確率分布が一致するときであるから、$p_g=p_{\text{data}}$のときであり、このときの$V(G)$の最小値は$-\log4$である。

% 以上の内容から、GANの理想的最適点では生成分布$p_g$と実データ$p_{\text{data}}$が一致し、その結果、識別器はすべての入力に対して$1/2$を出力する。

以上の内容から、識別器を最適解$D^*_G(x)=1/2$の近傍で保った状態で、生成器に対してJSダイバージェンスの最小化を目指すことで、唯一の最適解$p_g=p_{\text{data}}$に収束することが分かる。
\newpage
\subsubsection{GANの学習の流れ}
\eref{eq:minimax}をもとに、DiscriminatorおよびGeneratorの損失関数を$L_D、L_G$とすると、\eref{eq:LD}、\eref{eq:LG}のようになる。
{\small
\begin{align}
    L_D&=\frac{1}{m}\sum_{i=1}^{m}\Bigl[\log D(x)+\log(1-D(G(z)))\Bigr]\label{eq:LD}\\
    L_G&=\frac{1}{m}\sum_{i=1}^{m}\log (1-D(G(z)))\label{eq:LG}
\end{align}
}
\subsubsection*{BCELoss(Binary Cross Entropy Loss)}
交差エントロピー損失(BCELoss)は\eref{eq:BCE}で定義される。
\begin{align}
    BCE(x,y) = -(y\log x + (1-y)\log(1-x)) \label{eq:BCE}
\end{align}

\subsubsection*{Discriminatorの学習}
Discriminatorの学習時の概要図を\fref{fg:D_train}に示す。Discriminator学習時はGeneratorのパラメータ$\theta_g$は固定する。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=65mm,keepaspectratio]{figure/D_train.png}
	\caption{Discrminatorの学習時の概要図}
	\label{fg:D_train}
\end{figure}

Discriminatorの目標は$p_{\mathrm{data}}$と$p_g$を判別することであるから、学習時のラベルとしてFake=0, True=1とした場合、$D(G(z))=0,D(x)=1$となる。

\eref{eq:BCE}を参考にすると、\eref{eq:LD}の損失は次式で表せる。
\begin{align}
    &BCE(D(x\sim p_{\text{data}}),1)=-\log(D(x))\notag\\
    &BCE(D(G(z)),0)=-\log(1-D(G(z)))\notag\\
    &L_D=|BCE(D(x),1)+BCE(D(G(z)),0)|
\end{align}


\subsubsection*{Generatorの学習}
Generatorの学習時の概要図を\fref{fg:D_train}に示す。Generator学習時はDiscriminatorのパラメータ$\theta_d$は固定する。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=65mm,keepaspectratio]{figure/G_train.png}
	\caption{Generatorの学習時の概要図}
	\label{fg:G_train}
\end{figure}

Generatorの目標は$p_{\mathrm{data}}=p_g$として、Discriminatorをだます分布を生成することであるから$D(G(z))=1$を目指す。\eref{eq:BCE}を参考にすると、\eref{eq:LG}の損失は次式で表せる。
\begin{align}
    &BCE(G(z),1)=-\log(D(x))\notag\\
    &L_G = |BCE(G(z),1)|
\end{align}    



\subsubsection*{学習アルゴリズム}
GANにおける学習アルゴリズムをAlgorithm\ref{alg:gan_train}に示す。
\begin{algorithm}
\caption{Training of GAN}
\label{alg:gan_train}
{\small
    \begin{algorithmic}[1]
    \State \textbf{Input:} correct Image $p_{\mathrm{data}}(x)$, noise prior $z \sim \mathcal{N}(0, I)$, minibatch size $m$
    \State \textbf{Input:}learning rates $\eta_D,\eta_G$
    \State Initialize parameters $\theta_d, \theta_g$
    \For{iteration $=1,2,\ldots$}
        
        \State Sample minibatch $\{x^{(i)}\}_{i=1}^{m} \sim p_{\mathrm{data}}(x)$
        \State Sample minibatch $\{z^{(i)}\}_{i=1}^{m} \sim p_z(z)$
        \State $\tilde{x}^{(i)} \gets G(z^{(i)};\theta_g)$
        \State $g_d \gets \nabla_{\theta_d}\frac{1}{m}\sum_{i=1}^{m}\Bigl[\log D(x^{(i)};\theta_d)+\log(1-D(\tilde{x}^{(i)};\theta_d))\Bigr]$
        \State $\theta_d \gets \theta_d + \eta_D\, g_d$
    

        \State Sample minibatch $\{z^{(i)}\}_{i=1}^{m} \sim p_z(z)$
        \State $\tilde{x}^{(i)} \gets G(z^{(i)};\theta_g)$
        \State $g_g \gets \nabla_{\theta_g}\frac{1}{m}\sum_{i=1}^{m}(1-\log D(\tilde{x}^{(i)};\theta_d))$ 
        \State $\theta_g \gets \theta_g - \eta_G\, g_g$
    \EndFor
    \end{algorithmic}
}
\end{algorithm}


\subsection{CNN}
本章では、GeneratorおよびDiscriminatorのネットワークとして使用したCNN(Convolutional Neural Network)について説明する。基礎となるニューラルネットワーク、誤差逆伝播法については補足資料2に記述した。

\subsubsection{CNNの概要}
CNNは画像認識のタスクにおいて優れた性能を示すことから注目される深層学習アルゴリズムである。CNNの概要図を\fref{fg:CNN1}に示す。画像には隣接するピクセル間に関係性があるため、CNNでは、「畳み込み層」「プーリング層」を用いて画像の局所的な特徴量（エッジや色の変化）を抽出する。分類問題では、最後に全結合層を用いることでスコアを算出する。


\begin{figure}[htbp]
	\centering
	\includegraphics[width=60mm,keepaspectratio]{figure/CNN1.png}
	\caption{CNNの概要図}
	\label{fg:CNN1}
\end{figure}

\subsection*{畳み込み層}
畳み込み層では、カーネルと呼ばれる小さなフィルタを入力データに対して畳み込むことで、局所的な特徴を抽出する。\fref{fg:CNN2}に示すように、複数種類のカーネルを入力データに適用することで、エッジや模様などの多様な画像特徴を捉えることができる。strideを1とした場合、カーネルは入力上を1マスずつスライドしながら、各位置において要素積の計算を行う。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=40mm,keepaspectratio]{figure/CNN2.png}
	\caption{畳み込み演算}
	\label{fg:CNN2}
\end{figure}

\subsection*{プーリング層}
プーリング層では、畳み込み層によって得られた特徴マップを空間的に縮小する処理を行う。代表的な手法としてマックスプーリングがあり、局所領域内の最大値を出力として採用する。(\fref{fg:CNN3})
\begin{figure}[htbp]
	\centering
	\includegraphics[width=40mm,keepaspectratio]{figure/CNN3.png}
	\caption{マックスプーリング}
	\label{fg:CNN3}
\end{figure}

\subsection{逆畳み込み演算}
CNNでは、情報を抽出するため畳み込み層を通過するごとに画像サイズは小さくなる。その一方で、逆畳み込み演算を用いることにより、画像を拡大していくことが可能となる。今回、Generatorは100次元の潜在変数ベクトルに対し、逆畳み込みを行うことで32×32の画像を生成する。逆畳み込みは以下の4つのステップに基づく。
\begin{enumerate}
    \item strideに応じたデータ拡張
    \item ゼロパディング
    \item paddingに応じた、余白の削除
    \item 畳み込み演算
\end{enumerate}

\subsubsection*{STEP1：strideに応じたデータ拡張}
\fref{fg:CNN4}に示すように、strideで指定した行数分だけ入力データのピクセル間に0を追加する。
\begin{figure}[htbp]
	\centering
	\includegraphics[width=40mm,keepaspectratio]{figure/CNN4.png}
	\caption{strideに応じたデータ拡張}
	\label{fg:CNN4}
\end{figure}

\subsubsection*{STEP2：ゼロパディング}
STEP2では、カーネルのサイズよりも1行少ない数だけ、余白の追加を行う。
\begin{figure}[htbp]
	\centering
	\includegraphics[width=40mm,keepaspectratio]{figure/CNN5.png}
	\caption{ゼロパディング}
	\label{fg:CNN5}
\end{figure}

\subsubsection*{STEP3：paddingに応じた、余白の削除}
paddingで指定した行数分の余白を削除する。
\begin{figure}[htbp]
	\centering
	\includegraphics[width=40mm,keepaspectratio]{figure/CNN6.png}
	\caption{paddingに応じた、余白の削除}
	\label{fg:CNN6}
\end{figure}
\subsubsection*{STEP4：畳み込み演算}
\fref{fg:CNN2}と同様の、通常の畳み込み演算を行う。



